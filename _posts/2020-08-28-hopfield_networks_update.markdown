---
layout: post
title:  "Hopfield Networks are not that useless."
date:   2020-08-28 10:18:14 +0100
categories: neural networks
---

To my surprise, there has been a recent interest in my Hopfield Networks wrap-up
[page](_posts/2020-08-28-hopfield_networks_update.markdown).

For the record, it is also available on [Medium](https://towardsdatascience.com/hopfield-networks-are-useless-heres-why-you-should-learn-them-f0930ebeadcd),
where a [fellow reader](https://medium.com/@dinu.marius.constantin) pointed me
to [this article](https://arxiv.org/abs/2008.02217),
titled "Hopfield Networks is all you need". As he jokingly noticed, it turns out
that Hopfield Networks are not that useless after all, but it still makes for a
catchy title. And speaking of titles, let's be pedantic and notice how it should
have been "Hopfield Network *are* all you need". Naturally, it is a reference to
the ["Attention is all you need"](https://arxiv.org/abs/1706.03762) paper on Transformer
architectures, so I guess we are both guilty of sacrificing clarity on the altar of
clickbaiting.

 Who would have thought.